{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65cca90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec000ac",
   "metadata": {},
   "source": [
    "# CBAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4dd8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Channel_Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv2d(channels,channels,1,1,0,bias=True)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        return x*self.act(self.conv(self.pool(x)))\n",
    "    \n",
    "class Spatial_Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, kernal_size = 7):\n",
    "        super().__init__()\n",
    "        assert kernal_size in {3,7}, \"kernal size must be (3 or 7)\"\n",
    "        padding = 3 if kernal_size == 7 else 1\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernal_size,padding=padding, bias=False)\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_x = torch.mean(x, 1, keepdim=True)\n",
    "        max_x = torch.max(x, 1, keepdim=True)[0]\n",
    "        cat_x = torch.cat([avg_x,max_x],1)\n",
    "\n",
    "        return x * self.act(self.conv(cat_x))\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "\n",
    "    def __init__(self, c1, kernal_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_attention = Channel_Attention(c1)\n",
    "        self.spatial_attention = Spatial_Attention(kernal_size=kernal_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.spatial_attention(self.channel_attention(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7990ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004258 million parameters\n",
      "CBAM input shape:  torch.Size([1, 64, 256, 256])\n",
      "CBAM Output shape:  torch.Size([1, 64, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "cbam = CBAM(c1=64, kernal_size=7)\n",
    "print(f\"{sum(p.numel() for p in cbam.parameters())/1e6} million parameters\")\n",
    "dummy_input = torch.rand((1,64,256,256))\n",
    "dummy_output = cbam(dummy_input)\n",
    "print(\"CBAM input shape: \",dummy_input.shape)\n",
    "print(\"CBAM Output shape: \",dummy_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0497902b",
   "metadata": {},
   "source": [
    "# ASPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2594f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPP_Lite(nn.Module):\n",
    "    \n",
    "    def __init__(self,c1,dilations=(1, 6, 12, 18), reduction=4):\n",
    "        super().__init__()\n",
    "        c2 = c1\n",
    "        c_ = c2 // reduction\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(c1, c_, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(c_),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.branches = nn.ModuleList()\n",
    "        for d in dilations:\n",
    "            self.branches.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(c1, c_, kernel_size=3, padding=d, dilation=d, bias=False),\n",
    "                    nn.BatchNorm2d(c_),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.global_conv = nn.Sequential(\n",
    "            nn.Conv2d(c1, c_, kernel_size=1, bias=False),\n",
    "            # nn.BatchNorm2d(c2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d((len(dilations)+2)*c_, c2 ,kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(c2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h, w = x.shape[2],x.shape[3]\n",
    "        out1 = self.branch1(x)\n",
    "        outs = [out1]\n",
    "\n",
    "        for branch in self.branches:\n",
    "            outs.append(branch(x))\n",
    "        \n",
    "        gp = self.global_pool(x)\n",
    "        gp = self.global_conv(gp)\n",
    "        gp = nn.functional.interpolate(gp, size=(h,w), mode='bilinear', align_corners=False)\n",
    "        outs.append(gp)\n",
    "\n",
    "        out = torch.cat(outs, dim=1)\n",
    "        return self.project(out)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f4d48e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 11.54M\n",
      "Input shape : torch.Size([1, 1024, 32, 32])\n",
      "Output shape: torch.Size([1, 1024, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "aspp = ASPP_Lite(c1=1024)\n",
    "print(f\"Parameters: {sum(p.numel() for p in aspp.parameters())/1e6:.2f}M\")\n",
    "\n",
    "dummy = torch.randn(1, 1024, 32, 32)\n",
    "out = aspp(dummy)\n",
    "print(\"Input shape :\", dummy.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc4a6b",
   "metadata": {},
   "source": [
    "# MobileVitBlock\n",
    "https://keras.io/examples/vision/mobilevit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class ConvBNAct(nn.Sequential):\n",
    "    def __init__(self, in_ch, out_ch, kernel=3, stride=1, padding=1, bias=False, act=True):\n",
    "        layers = [nn.Conv2d(in_ch, out_ch, kernel, stride, padding, bias=bias),\n",
    "                  nn.BatchNorm2d(out_ch)]\n",
    "        if act:\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        c1: int,\n",
    "        depth: int = 2,\n",
    "        patch_size: int = 2,\n",
    "        expansion: int = 2,\n",
    "        num_heads: Optional[int] = None,\n",
    "        mlp_ratio: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert patch_size >= 1 and isinstance(patch_size, int)\n",
    "        self.c_in = c1\n",
    "        self.depth = depth\n",
    "        self.patch_size = patch_size\n",
    "        # c_local = int(min( c1 * expansion, 2048 ))  # small expansion\n",
    "        c_local = c1//4\n",
    "        self.local_feat = ConvBNAct(c1, c_local, kernel=3, padding=1)\n",
    "        self.project_to_tokens = nn.Sequential(\n",
    "            nn.Conv2d(c_local, c_local, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(c_local),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        embed_dim = c_local\n",
    "        \n",
    "        if num_heads is None:\n",
    "           \n",
    "            if embed_dim % 8 == 0:\n",
    "                num_heads = 8\n",
    "            elif embed_dim % 4 == 0:\n",
    "                num_heads = 4\n",
    "            else:\n",
    "               \n",
    "                num_heads = 1\n",
    "        self.num_heads = num_heads\n",
    "       \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * mlp_ratio,\n",
    "            batch_first=True,\n",
    "            activation='relu',\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        \n",
    "        self.project_from_tokens = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "       \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(c_local + embed_dim, c1, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(c1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ConvBNAct(c1, c1, kernel=3, padding=1)  \n",
    "        )\n",
    "\n",
    "    def _pad_to_multiple(self, x, multiple):\n",
    "        \n",
    "        _, _, h, w = x.shape\n",
    "        pad_h = (multiple - (h % multiple)) % multiple\n",
    "        pad_w = (multiple - (w % multiple)) % multiple\n",
    "        if pad_h == 0 and pad_w == 0:\n",
    "            return x, (0, 0)\n",
    "        \n",
    "        x = F.pad(x, (0, pad_w, 0, pad_h))\n",
    "        return x, (pad_h, pad_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape        \n",
    "        local = self.local_feat(x) \n",
    "        proj = self.project_to_tokens(local)  \n",
    "        proj, (pad_h, pad_w) = self._pad_to_multiple(proj, self.patch_size)\n",
    "        _, Cproj, Hp, Wp = proj.shape\n",
    "        Ph = self.patch_size\n",
    "        Pw = self.patch_size\n",
    "        nh = Hp // Ph\n",
    "        nw = Wp // Pw\n",
    "        num_patches = nh * nw\n",
    "        proj = proj.view(B, Cproj, nh, Ph, nw, Pw)\n",
    "        proj = proj.permute(0, 2, 4, 3, 5, 1).contiguous()  \n",
    "        proj = proj.view(B, num_patches, Ph * Pw * Cproj) \n",
    "        if proj.shape[-1] != Cproj:\n",
    "            \n",
    "            proj = proj.view(B * num_patches, -1)\n",
    "            proj = nn.functional.adaptive_avg_pool1d(proj.unsqueeze(1), Cproj).squeeze(1)\n",
    "            proj = proj.view(B, num_patches, Cproj)\n",
    "        \n",
    "        tokens = proj  \n",
    "        tokens = self.transformer(tokens)  \n",
    "        tokens = tokens.view(B, nh, nw, Cproj) \n",
    "        tokens = tokens.permute(0, 3, 1, 2).contiguous()  # (B, embed_dim, nh, nw)\n",
    "        tokens = tokens.unsqueeze(-1).unsqueeze(-1)  \n",
    "        tokens = tokens.repeat(1, 1, 1, 1, Ph, Pw)  \n",
    "        tokens = tokens.permute(0, 1, 2, 4, 3, 5).contiguous() \n",
    "        tokens = tokens.view(B, Cproj, Hp, Wp) \n",
    "        tokens = self.project_from_tokens(tokens)  \n",
    "\n",
    "        if pad_h != 0 or pad_w != 0:\n",
    "            tokens = tokens[:, :, :H, :W]\n",
    "            local = local[:, :, :H, :W]\n",
    "        fused = torch.cat([local, tokens], dim=1)\n",
    "        out = self.fusion(fused)\n",
    "        if out.shape == x.shape:\n",
    "            out = out + x\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf15c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 3.381M\n",
      "Input shape : torch.Size([1, 512, 32, 32])\n",
      "Output shape: torch.Size([1, 512, 32, 32])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mvit = MobileViTBlock(512, depth=2, patch_size=2)\n",
    "print(f\"Parameters: {sum(p.numel() for p in mvit.parameters())/1e6:.3f}M\")\n",
    "\n",
    "dummy = torch.randn(1, 512, 32, 32)\n",
    "out = mvit(dummy)\n",
    "print(\"Input shape :\", dummy.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee06856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
